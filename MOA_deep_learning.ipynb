{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "dlcourse-venv",
      "language": "python",
      "name": "dlcourse-venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "Copy of MOA_deep_learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThomLlobEce/Portfolio/blob/master/MOA_deep_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_k2m3Bqumrq"
      },
      "source": [
        "# Mechanisms of Action (Moa) Prediction\n",
        "\n",
        "Ece 2020/2021 Ing5 BDA grp1:\n",
        "\n",
        "\n",
        "*   Antoniadis Pablo\n",
        "*   Llobregat Thomas\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iwVL2W_p8EF",
        "outputId": "653e9a3c-fecd-4e06-9f2f-55fa912d6e72"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Make numpy values easier to read.\n",
        "np.set_printoptions(precision=3, suppress=True)\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "from tensorboard.plugins.hparams import api as hp\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xD6OsH_8p8EF"
      },
      "source": [
        "BATCH_SIZE = 32"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8Sb-Z4vp8EF"
      },
      "source": [
        "# Read metadata about our data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvYH4h61p8EF",
        "outputId": "d7975042-2d97-4270-c18e-9fb31665b8ba"
      },
      "source": [
        "features = pd.read_csv(\"/content/drive/MyDrive/Ing5/Deep-learning/train_features.csv\", nrows=10)\n",
        "targets = pd.read_csv(\"/content/drive/MyDrive/Ing5/Deep-learning/train_targets_scored.csv\", nrows=10)\n",
        "\n",
        "cols_features = features.columns\n",
        "cols_targets = targets.columns\n",
        "\n",
        "num_features = len(cols_features)\n",
        "num_targets = len(cols_targets)\n",
        "\n",
        "print(\"Number of features:\" , num_features)\n",
        "print(\"Number of targets:\" , num_targets)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of features: 876\n",
            "Number of targets: 207\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "-tFeGQfap8EG",
        "outputId": "f0cd49f9-4dc4-45d7-d8d7-e5565ca61d8b"
      },
      "source": [
        "features.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sig_id</th>\n",
              "      <th>cp_type</th>\n",
              "      <th>cp_time</th>\n",
              "      <th>cp_dose</th>\n",
              "      <th>g-0</th>\n",
              "      <th>g-1</th>\n",
              "      <th>g-2</th>\n",
              "      <th>g-3</th>\n",
              "      <th>g-4</th>\n",
              "      <th>g-5</th>\n",
              "      <th>g-6</th>\n",
              "      <th>g-7</th>\n",
              "      <th>g-8</th>\n",
              "      <th>g-9</th>\n",
              "      <th>g-10</th>\n",
              "      <th>g-11</th>\n",
              "      <th>g-12</th>\n",
              "      <th>g-13</th>\n",
              "      <th>g-14</th>\n",
              "      <th>g-15</th>\n",
              "      <th>g-16</th>\n",
              "      <th>g-17</th>\n",
              "      <th>g-18</th>\n",
              "      <th>g-19</th>\n",
              "      <th>g-20</th>\n",
              "      <th>g-21</th>\n",
              "      <th>g-22</th>\n",
              "      <th>g-23</th>\n",
              "      <th>g-24</th>\n",
              "      <th>g-25</th>\n",
              "      <th>g-26</th>\n",
              "      <th>g-27</th>\n",
              "      <th>g-28</th>\n",
              "      <th>g-29</th>\n",
              "      <th>g-30</th>\n",
              "      <th>g-31</th>\n",
              "      <th>g-32</th>\n",
              "      <th>g-33</th>\n",
              "      <th>g-34</th>\n",
              "      <th>g-35</th>\n",
              "      <th>...</th>\n",
              "      <th>c-60</th>\n",
              "      <th>c-61</th>\n",
              "      <th>c-62</th>\n",
              "      <th>c-63</th>\n",
              "      <th>c-64</th>\n",
              "      <th>c-65</th>\n",
              "      <th>c-66</th>\n",
              "      <th>c-67</th>\n",
              "      <th>c-68</th>\n",
              "      <th>c-69</th>\n",
              "      <th>c-70</th>\n",
              "      <th>c-71</th>\n",
              "      <th>c-72</th>\n",
              "      <th>c-73</th>\n",
              "      <th>c-74</th>\n",
              "      <th>c-75</th>\n",
              "      <th>c-76</th>\n",
              "      <th>c-77</th>\n",
              "      <th>c-78</th>\n",
              "      <th>c-79</th>\n",
              "      <th>c-80</th>\n",
              "      <th>c-81</th>\n",
              "      <th>c-82</th>\n",
              "      <th>c-83</th>\n",
              "      <th>c-84</th>\n",
              "      <th>c-85</th>\n",
              "      <th>c-86</th>\n",
              "      <th>c-87</th>\n",
              "      <th>c-88</th>\n",
              "      <th>c-89</th>\n",
              "      <th>c-90</th>\n",
              "      <th>c-91</th>\n",
              "      <th>c-92</th>\n",
              "      <th>c-93</th>\n",
              "      <th>c-94</th>\n",
              "      <th>c-95</th>\n",
              "      <th>c-96</th>\n",
              "      <th>c-97</th>\n",
              "      <th>c-98</th>\n",
              "      <th>c-99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>id_000644bb2</td>\n",
              "      <td>trt_cp</td>\n",
              "      <td>24</td>\n",
              "      <td>D1</td>\n",
              "      <td>1.0620</td>\n",
              "      <td>0.5577</td>\n",
              "      <td>-0.2479</td>\n",
              "      <td>-0.6208</td>\n",
              "      <td>-0.1944</td>\n",
              "      <td>-1.0120</td>\n",
              "      <td>-1.0220</td>\n",
              "      <td>-0.0326</td>\n",
              "      <td>0.5548</td>\n",
              "      <td>-0.0921</td>\n",
              "      <td>1.1830</td>\n",
              "      <td>0.1530</td>\n",
              "      <td>0.5574</td>\n",
              "      <td>-0.4015</td>\n",
              "      <td>0.1789</td>\n",
              "      <td>-0.6528</td>\n",
              "      <td>-0.7969</td>\n",
              "      <td>0.6342</td>\n",
              "      <td>0.1778</td>\n",
              "      <td>-0.3694</td>\n",
              "      <td>-0.5688</td>\n",
              "      <td>-1.1360</td>\n",
              "      <td>-1.1880</td>\n",
              "      <td>0.6940</td>\n",
              "      <td>0.4393</td>\n",
              "      <td>0.2664</td>\n",
              "      <td>0.1907</td>\n",
              "      <td>0.1628</td>\n",
              "      <td>-0.2853</td>\n",
              "      <td>0.5819</td>\n",
              "      <td>0.2934</td>\n",
              "      <td>-0.5584</td>\n",
              "      <td>-0.0916</td>\n",
              "      <td>-0.3010</td>\n",
              "      <td>-0.1537</td>\n",
              "      <td>0.2198</td>\n",
              "      <td>...</td>\n",
              "      <td>0.4805</td>\n",
              "      <td>0.4965</td>\n",
              "      <td>0.3680</td>\n",
              "      <td>0.8427</td>\n",
              "      <td>0.1042</td>\n",
              "      <td>0.1403</td>\n",
              "      <td>0.1758</td>\n",
              "      <td>1.2570</td>\n",
              "      <td>-0.5979</td>\n",
              "      <td>1.2250</td>\n",
              "      <td>-0.0553</td>\n",
              "      <td>0.7351</td>\n",
              "      <td>0.5810</td>\n",
              "      <td>0.9590</td>\n",
              "      <td>0.2427</td>\n",
              "      <td>0.0495</td>\n",
              "      <td>0.4141</td>\n",
              "      <td>0.8432</td>\n",
              "      <td>0.6162</td>\n",
              "      <td>-0.7318</td>\n",
              "      <td>1.2120</td>\n",
              "      <td>0.6362</td>\n",
              "      <td>-0.4427</td>\n",
              "      <td>0.1288</td>\n",
              "      <td>1.4840</td>\n",
              "      <td>0.1799</td>\n",
              "      <td>0.5367</td>\n",
              "      <td>-0.1111</td>\n",
              "      <td>-1.0120</td>\n",
              "      <td>0.6685</td>\n",
              "      <td>0.2862</td>\n",
              "      <td>0.2584</td>\n",
              "      <td>0.8076</td>\n",
              "      <td>0.5523</td>\n",
              "      <td>-0.1912</td>\n",
              "      <td>0.6584</td>\n",
              "      <td>-0.3981</td>\n",
              "      <td>0.2139</td>\n",
              "      <td>0.3801</td>\n",
              "      <td>0.4176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>id_000779bfc</td>\n",
              "      <td>trt_cp</td>\n",
              "      <td>72</td>\n",
              "      <td>D1</td>\n",
              "      <td>0.0743</td>\n",
              "      <td>0.4087</td>\n",
              "      <td>0.2991</td>\n",
              "      <td>0.0604</td>\n",
              "      <td>1.0190</td>\n",
              "      <td>0.5207</td>\n",
              "      <td>0.2341</td>\n",
              "      <td>0.3372</td>\n",
              "      <td>-0.4047</td>\n",
              "      <td>0.8507</td>\n",
              "      <td>-1.1520</td>\n",
              "      <td>-0.4201</td>\n",
              "      <td>-0.0958</td>\n",
              "      <td>0.4590</td>\n",
              "      <td>0.0803</td>\n",
              "      <td>0.2250</td>\n",
              "      <td>0.5293</td>\n",
              "      <td>0.2839</td>\n",
              "      <td>-0.3494</td>\n",
              "      <td>0.2883</td>\n",
              "      <td>0.9449</td>\n",
              "      <td>-0.1646</td>\n",
              "      <td>-0.2657</td>\n",
              "      <td>-0.3372</td>\n",
              "      <td>0.3135</td>\n",
              "      <td>-0.4316</td>\n",
              "      <td>0.4773</td>\n",
              "      <td>0.2075</td>\n",
              "      <td>-0.4216</td>\n",
              "      <td>-0.1161</td>\n",
              "      <td>-0.0499</td>\n",
              "      <td>-0.2627</td>\n",
              "      <td>0.9959</td>\n",
              "      <td>-0.2483</td>\n",
              "      <td>0.2655</td>\n",
              "      <td>-0.2102</td>\n",
              "      <td>...</td>\n",
              "      <td>0.4083</td>\n",
              "      <td>0.0319</td>\n",
              "      <td>0.3905</td>\n",
              "      <td>0.7099</td>\n",
              "      <td>0.2912</td>\n",
              "      <td>0.4151</td>\n",
              "      <td>-0.2840</td>\n",
              "      <td>-0.3104</td>\n",
              "      <td>-0.6373</td>\n",
              "      <td>0.2887</td>\n",
              "      <td>-0.0765</td>\n",
              "      <td>0.2539</td>\n",
              "      <td>0.4443</td>\n",
              "      <td>0.5932</td>\n",
              "      <td>0.2031</td>\n",
              "      <td>0.7639</td>\n",
              "      <td>0.5499</td>\n",
              "      <td>-0.3322</td>\n",
              "      <td>-0.0977</td>\n",
              "      <td>0.4329</td>\n",
              "      <td>-0.2782</td>\n",
              "      <td>0.7827</td>\n",
              "      <td>0.5934</td>\n",
              "      <td>0.3402</td>\n",
              "      <td>0.1499</td>\n",
              "      <td>0.4420</td>\n",
              "      <td>0.9366</td>\n",
              "      <td>0.8193</td>\n",
              "      <td>-0.4236</td>\n",
              "      <td>0.3192</td>\n",
              "      <td>-0.4265</td>\n",
              "      <td>0.7543</td>\n",
              "      <td>0.4708</td>\n",
              "      <td>0.0230</td>\n",
              "      <td>0.2957</td>\n",
              "      <td>0.4899</td>\n",
              "      <td>0.1522</td>\n",
              "      <td>0.1241</td>\n",
              "      <td>0.6077</td>\n",
              "      <td>0.7371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>id_000a6266a</td>\n",
              "      <td>trt_cp</td>\n",
              "      <td>48</td>\n",
              "      <td>D1</td>\n",
              "      <td>0.6280</td>\n",
              "      <td>0.5817</td>\n",
              "      <td>1.5540</td>\n",
              "      <td>-0.0764</td>\n",
              "      <td>-0.0323</td>\n",
              "      <td>1.2390</td>\n",
              "      <td>0.1715</td>\n",
              "      <td>0.2155</td>\n",
              "      <td>0.0065</td>\n",
              "      <td>1.2300</td>\n",
              "      <td>-0.4797</td>\n",
              "      <td>-0.5631</td>\n",
              "      <td>-0.0366</td>\n",
              "      <td>-1.8300</td>\n",
              "      <td>0.6057</td>\n",
              "      <td>-0.3278</td>\n",
              "      <td>0.6042</td>\n",
              "      <td>-0.3075</td>\n",
              "      <td>-0.1147</td>\n",
              "      <td>-0.0570</td>\n",
              "      <td>-0.0799</td>\n",
              "      <td>-0.8181</td>\n",
              "      <td>-1.5320</td>\n",
              "      <td>0.2307</td>\n",
              "      <td>0.4901</td>\n",
              "      <td>0.4780</td>\n",
              "      <td>-1.3970</td>\n",
              "      <td>4.6240</td>\n",
              "      <td>-0.0437</td>\n",
              "      <td>1.2870</td>\n",
              "      <td>-1.8530</td>\n",
              "      <td>0.6069</td>\n",
              "      <td>0.4290</td>\n",
              "      <td>0.1783</td>\n",
              "      <td>0.0018</td>\n",
              "      <td>-1.1800</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.5477</td>\n",
              "      <td>-0.7576</td>\n",
              "      <td>-0.0444</td>\n",
              "      <td>0.1894</td>\n",
              "      <td>-0.0014</td>\n",
              "      <td>-2.3640</td>\n",
              "      <td>-0.4682</td>\n",
              "      <td>0.1210</td>\n",
              "      <td>-0.5177</td>\n",
              "      <td>-0.0604</td>\n",
              "      <td>0.1682</td>\n",
              "      <td>-0.4436</td>\n",
              "      <td>0.4963</td>\n",
              "      <td>0.1363</td>\n",
              "      <td>0.3335</td>\n",
              "      <td>0.9760</td>\n",
              "      <td>-0.0427</td>\n",
              "      <td>-0.1235</td>\n",
              "      <td>0.0959</td>\n",
              "      <td>0.0690</td>\n",
              "      <td>-0.9416</td>\n",
              "      <td>-0.7548</td>\n",
              "      <td>-0.1109</td>\n",
              "      <td>-0.6272</td>\n",
              "      <td>0.3019</td>\n",
              "      <td>0.1172</td>\n",
              "      <td>0.1093</td>\n",
              "      <td>-0.3113</td>\n",
              "      <td>0.3019</td>\n",
              "      <td>-0.0873</td>\n",
              "      <td>-0.7250</td>\n",
              "      <td>-0.6297</td>\n",
              "      <td>0.6103</td>\n",
              "      <td>0.0223</td>\n",
              "      <td>-1.3240</td>\n",
              "      <td>-0.3174</td>\n",
              "      <td>-0.6417</td>\n",
              "      <td>-0.2187</td>\n",
              "      <td>-1.4080</td>\n",
              "      <td>0.6931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>id_0015fd391</td>\n",
              "      <td>trt_cp</td>\n",
              "      <td>48</td>\n",
              "      <td>D1</td>\n",
              "      <td>-0.5138</td>\n",
              "      <td>-0.2491</td>\n",
              "      <td>-0.2656</td>\n",
              "      <td>0.5288</td>\n",
              "      <td>4.0620</td>\n",
              "      <td>-0.8095</td>\n",
              "      <td>-1.9590</td>\n",
              "      <td>0.1792</td>\n",
              "      <td>-0.1321</td>\n",
              "      <td>-1.0600</td>\n",
              "      <td>-0.8269</td>\n",
              "      <td>-0.3584</td>\n",
              "      <td>-0.8511</td>\n",
              "      <td>-0.5844</td>\n",
              "      <td>-2.5690</td>\n",
              "      <td>0.8183</td>\n",
              "      <td>-0.0532</td>\n",
              "      <td>-0.8554</td>\n",
              "      <td>0.1160</td>\n",
              "      <td>-2.3520</td>\n",
              "      <td>2.1200</td>\n",
              "      <td>-1.1580</td>\n",
              "      <td>-0.7191</td>\n",
              "      <td>-0.8004</td>\n",
              "      <td>-1.4670</td>\n",
              "      <td>-0.0107</td>\n",
              "      <td>-0.8995</td>\n",
              "      <td>0.2406</td>\n",
              "      <td>-0.2479</td>\n",
              "      <td>-1.0890</td>\n",
              "      <td>-0.7575</td>\n",
              "      <td>0.0881</td>\n",
              "      <td>-2.7370</td>\n",
              "      <td>0.8745</td>\n",
              "      <td>0.5787</td>\n",
              "      <td>-1.6740</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.1220</td>\n",
              "      <td>-0.3752</td>\n",
              "      <td>-2.3820</td>\n",
              "      <td>-3.7350</td>\n",
              "      <td>-2.9740</td>\n",
              "      <td>-1.4930</td>\n",
              "      <td>-1.6600</td>\n",
              "      <td>-3.1660</td>\n",
              "      <td>0.2816</td>\n",
              "      <td>-0.2990</td>\n",
              "      <td>-1.1870</td>\n",
              "      <td>-0.5044</td>\n",
              "      <td>-1.7750</td>\n",
              "      <td>-1.6120</td>\n",
              "      <td>-0.9215</td>\n",
              "      <td>-1.0810</td>\n",
              "      <td>-3.0520</td>\n",
              "      <td>-3.4470</td>\n",
              "      <td>-2.7740</td>\n",
              "      <td>-1.8460</td>\n",
              "      <td>-0.5568</td>\n",
              "      <td>-3.3960</td>\n",
              "      <td>-2.9510</td>\n",
              "      <td>-1.1550</td>\n",
              "      <td>-3.2620</td>\n",
              "      <td>-1.5390</td>\n",
              "      <td>-2.4600</td>\n",
              "      <td>-0.9417</td>\n",
              "      <td>-1.5550</td>\n",
              "      <td>0.2431</td>\n",
              "      <td>-2.0990</td>\n",
              "      <td>-0.6441</td>\n",
              "      <td>-5.6300</td>\n",
              "      <td>-1.3780</td>\n",
              "      <td>-0.8632</td>\n",
              "      <td>-1.2880</td>\n",
              "      <td>-1.6210</td>\n",
              "      <td>-0.8784</td>\n",
              "      <td>-0.3876</td>\n",
              "      <td>-0.8154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>id_001626bd3</td>\n",
              "      <td>trt_cp</td>\n",
              "      <td>72</td>\n",
              "      <td>D2</td>\n",
              "      <td>-0.3254</td>\n",
              "      <td>-0.4009</td>\n",
              "      <td>0.9700</td>\n",
              "      <td>0.6919</td>\n",
              "      <td>1.4180</td>\n",
              "      <td>-0.8244</td>\n",
              "      <td>-0.2800</td>\n",
              "      <td>-0.1498</td>\n",
              "      <td>-0.8789</td>\n",
              "      <td>0.8630</td>\n",
              "      <td>-0.2219</td>\n",
              "      <td>-0.5121</td>\n",
              "      <td>-0.9577</td>\n",
              "      <td>1.1750</td>\n",
              "      <td>0.2042</td>\n",
              "      <td>0.1970</td>\n",
              "      <td>0.1244</td>\n",
              "      <td>-1.7090</td>\n",
              "      <td>-0.3543</td>\n",
              "      <td>-0.5160</td>\n",
              "      <td>-0.3330</td>\n",
              "      <td>-0.2685</td>\n",
              "      <td>0.7649</td>\n",
              "      <td>0.2057</td>\n",
              "      <td>1.3720</td>\n",
              "      <td>0.6835</td>\n",
              "      <td>0.8056</td>\n",
              "      <td>-0.3754</td>\n",
              "      <td>-1.2090</td>\n",
              "      <td>0.2965</td>\n",
              "      <td>-0.0712</td>\n",
              "      <td>0.6389</td>\n",
              "      <td>0.6674</td>\n",
              "      <td>-0.0783</td>\n",
              "      <td>1.1740</td>\n",
              "      <td>-0.7110</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.2274</td>\n",
              "      <td>0.3215</td>\n",
              "      <td>0.1535</td>\n",
              "      <td>-0.4640</td>\n",
              "      <td>-0.5943</td>\n",
              "      <td>0.3973</td>\n",
              "      <td>0.1500</td>\n",
              "      <td>0.5178</td>\n",
              "      <td>0.5159</td>\n",
              "      <td>0.6091</td>\n",
              "      <td>0.1813</td>\n",
              "      <td>-0.4249</td>\n",
              "      <td>0.7832</td>\n",
              "      <td>0.6529</td>\n",
              "      <td>0.5648</td>\n",
              "      <td>0.4817</td>\n",
              "      <td>0.0587</td>\n",
              "      <td>0.5303</td>\n",
              "      <td>0.6376</td>\n",
              "      <td>-0.3966</td>\n",
              "      <td>-1.4950</td>\n",
              "      <td>-0.9625</td>\n",
              "      <td>-0.0541</td>\n",
              "      <td>0.6273</td>\n",
              "      <td>0.4563</td>\n",
              "      <td>0.0698</td>\n",
              "      <td>0.8134</td>\n",
              "      <td>0.1924</td>\n",
              "      <td>0.6054</td>\n",
              "      <td>-0.1824</td>\n",
              "      <td>0.0042</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.6670</td>\n",
              "      <td>1.0690</td>\n",
              "      <td>0.5523</td>\n",
              "      <td>-0.3031</td>\n",
              "      <td>0.1094</td>\n",
              "      <td>0.2885</td>\n",
              "      <td>-0.3786</td>\n",
              "      <td>0.7125</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 876 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         sig_id cp_type  cp_time cp_dose  ...    c-96    c-97    c-98    c-99\n",
              "0  id_000644bb2  trt_cp       24      D1  ... -0.3981  0.2139  0.3801  0.4176\n",
              "1  id_000779bfc  trt_cp       72      D1  ...  0.1522  0.1241  0.6077  0.7371\n",
              "2  id_000a6266a  trt_cp       48      D1  ... -0.6417 -0.2187 -1.4080  0.6931\n",
              "3  id_0015fd391  trt_cp       48      D1  ... -1.6210 -0.8784 -0.3876 -0.8154\n",
              "4  id_001626bd3  trt_cp       72      D2  ...  0.1094  0.2885 -0.3786  0.7125\n",
              "\n",
              "[5 rows x 876 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "U9g32E9Dp8EG",
        "outputId": "9448f012-566d-46b0-fab2-49b719f85081"
      },
      "source": [
        "targets.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sig_id</th>\n",
              "      <th>5-alpha_reductase_inhibitor</th>\n",
              "      <th>11-beta-hsd1_inhibitor</th>\n",
              "      <th>acat_inhibitor</th>\n",
              "      <th>acetylcholine_receptor_agonist</th>\n",
              "      <th>acetylcholine_receptor_antagonist</th>\n",
              "      <th>acetylcholinesterase_inhibitor</th>\n",
              "      <th>adenosine_receptor_agonist</th>\n",
              "      <th>adenosine_receptor_antagonist</th>\n",
              "      <th>adenylyl_cyclase_activator</th>\n",
              "      <th>adrenergic_receptor_agonist</th>\n",
              "      <th>adrenergic_receptor_antagonist</th>\n",
              "      <th>akt_inhibitor</th>\n",
              "      <th>aldehyde_dehydrogenase_inhibitor</th>\n",
              "      <th>alk_inhibitor</th>\n",
              "      <th>ampk_activator</th>\n",
              "      <th>analgesic</th>\n",
              "      <th>androgen_receptor_agonist</th>\n",
              "      <th>androgen_receptor_antagonist</th>\n",
              "      <th>anesthetic_-_local</th>\n",
              "      <th>angiogenesis_inhibitor</th>\n",
              "      <th>angiotensin_receptor_antagonist</th>\n",
              "      <th>anti-inflammatory</th>\n",
              "      <th>antiarrhythmic</th>\n",
              "      <th>antibiotic</th>\n",
              "      <th>anticonvulsant</th>\n",
              "      <th>antifungal</th>\n",
              "      <th>antihistamine</th>\n",
              "      <th>antimalarial</th>\n",
              "      <th>antioxidant</th>\n",
              "      <th>antiprotozoal</th>\n",
              "      <th>antiviral</th>\n",
              "      <th>apoptosis_stimulant</th>\n",
              "      <th>aromatase_inhibitor</th>\n",
              "      <th>atm_kinase_inhibitor</th>\n",
              "      <th>atp-sensitive_potassium_channel_antagonist</th>\n",
              "      <th>atp_synthase_inhibitor</th>\n",
              "      <th>atpase_inhibitor</th>\n",
              "      <th>atr_kinase_inhibitor</th>\n",
              "      <th>aurora_kinase_inhibitor</th>\n",
              "      <th>...</th>\n",
              "      <th>protein_synthesis_inhibitor</th>\n",
              "      <th>protein_tyrosine_kinase_inhibitor</th>\n",
              "      <th>radiopaque_medium</th>\n",
              "      <th>raf_inhibitor</th>\n",
              "      <th>ras_gtpase_inhibitor</th>\n",
              "      <th>retinoid_receptor_agonist</th>\n",
              "      <th>retinoid_receptor_antagonist</th>\n",
              "      <th>rho_associated_kinase_inhibitor</th>\n",
              "      <th>ribonucleoside_reductase_inhibitor</th>\n",
              "      <th>rna_polymerase_inhibitor</th>\n",
              "      <th>serotonin_receptor_agonist</th>\n",
              "      <th>serotonin_receptor_antagonist</th>\n",
              "      <th>serotonin_reuptake_inhibitor</th>\n",
              "      <th>sigma_receptor_agonist</th>\n",
              "      <th>sigma_receptor_antagonist</th>\n",
              "      <th>smoothened_receptor_antagonist</th>\n",
              "      <th>sodium_channel_inhibitor</th>\n",
              "      <th>sphingosine_receptor_agonist</th>\n",
              "      <th>src_inhibitor</th>\n",
              "      <th>steroid</th>\n",
              "      <th>syk_inhibitor</th>\n",
              "      <th>tachykinin_antagonist</th>\n",
              "      <th>tgf-beta_receptor_inhibitor</th>\n",
              "      <th>thrombin_inhibitor</th>\n",
              "      <th>thymidylate_synthase_inhibitor</th>\n",
              "      <th>tlr_agonist</th>\n",
              "      <th>tlr_antagonist</th>\n",
              "      <th>tnf_inhibitor</th>\n",
              "      <th>topoisomerase_inhibitor</th>\n",
              "      <th>transient_receptor_potential_channel_antagonist</th>\n",
              "      <th>tropomyosin_receptor_kinase_inhibitor</th>\n",
              "      <th>trpv_agonist</th>\n",
              "      <th>trpv_antagonist</th>\n",
              "      <th>tubulin_inhibitor</th>\n",
              "      <th>tyrosine_kinase_inhibitor</th>\n",
              "      <th>ubiquitin_specific_protease_inhibitor</th>\n",
              "      <th>vegfr_inhibitor</th>\n",
              "      <th>vitamin_b</th>\n",
              "      <th>vitamin_d_receptor_agonist</th>\n",
              "      <th>wnt_inhibitor</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>id_000644bb2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>id_000779bfc</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>id_000a6266a</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>id_0015fd391</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>id_001626bd3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 207 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         sig_id  ...  wnt_inhibitor\n",
              "0  id_000644bb2  ...              0\n",
              "1  id_000779bfc  ...              0\n",
              "2  id_000a6266a  ...              0\n",
              "3  id_0015fd391  ...              0\n",
              "4  id_001626bd3  ...              0\n",
              "\n",
              "[5 rows x 207 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOdXdsd9p8EG"
      },
      "source": [
        "# Reading data using tf.data.experimental.CsvDataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNqmOkmfp8EI"
      },
      "source": [
        "features_types = [str(), str(), str(), str()] + [float()]*(num_features-4)\n",
        "targets_types = [str()] + [float()]*(num_targets-1)\n",
        "\n",
        "features = tf.data.experimental.CsvDataset(\"/content/drive/MyDrive/Ing5/Deep-learning/train_features.csv\",\n",
        "                                           record_defaults=features_types,\n",
        "                                           #select_cols\n",
        "                                           header=True)\n",
        "\n",
        "targets = tf.data.experimental.CsvDataset(\"/content/drive/MyDrive/Ing5/Deep-learning/train_targets_scored.csv\",\n",
        "                                          record_defaults=targets_types,\n",
        "                                          header=True)\n",
        "\n",
        "dataset = tf.data.Dataset.zip((features, targets))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsDUTMS0p8EI",
        "outputId": "3a4e38d7-6f9a-4264-d2f9-dd673a8fba84"
      },
      "source": [
        "# split dataset into train and val\n",
        "dataset_size = dataset.reduce(0, lambda x, _: x + 1).numpy()\n",
        "\n",
        "train_size = int(0.7*dataset_size)\n",
        "val_size = dataset_size - train_size\n",
        "\n",
        "train = dataset.take(train_size)\n",
        "val = dataset.skip(train_size)\n",
        "val = dataset.take(val_size)\n",
        "\n",
        "train_size = train.reduce(0, lambda x, _: x + 1).numpy()\n",
        "val_size = val.reduce(0, lambda x, _: x + 1).numpy()\n",
        "\n",
        "print(\"Full dataset size:\", dataset_size)\n",
        "print(\"Train dataset size:\", train_size)\n",
        "print(\"Val dataset size:\", val_size)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Full dataset size: 23814\n",
            "Train dataset size: 16669\n",
            "Val dataset size: 7145\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrP8UAuHp8EI"
      },
      "source": [
        "def _preprocess_line(features, targets):\n",
        "    # Pack the result into a dictionary\n",
        "    features = dict(zip(cols_features, features))\n",
        "    features.pop('sig_id')\n",
        "\n",
        "    targets = tf.stack(targets[1:])\n",
        "    \n",
        "    return features, targets\n",
        "\n",
        "train = train.map(_preprocess_line)\n",
        "train = train.batch(BATCH_SIZE)\n",
        "\n",
        "val = val.map(_preprocess_line)\n",
        "val = val.batch(BATCH_SIZE)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-zBKJPzp8EI",
        "outputId": "0a1cf58e-2b2c-4a46-9534-6e8010b06014"
      },
      "source": [
        "for feature_batch, label_batch in train.take(1):\n",
        "    print('First 5 features:', list(feature_batch.keys())[:5])\n",
        "    print('A batch of cp_types:', feature_batch['cp_type'].numpy())\n",
        "    print('A batch of cp_times:', feature_batch['cp_time'].numpy())\n",
        "    print('A batch of cp_dose:', feature_batch['cp_dose'].numpy())\n",
        "    print('A batch of targets:', label_batch.numpy() ) "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 5 features: ['cp_type', 'cp_time', 'cp_dose', 'g-0', 'g-1']\n",
            "A batch of cp_types: [b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp'\n",
            " b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp'\n",
            " b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp'\n",
            " b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp' b'ctl_vehicle' b'trt_cp'\n",
            " b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp' b'trt_cp']\n",
            "A batch of cp_times: [b'24' b'72' b'48' b'48' b'72' b'24' b'24' b'48' b'48' b'48' b'72' b'48'\n",
            " b'48' b'48' b'72' b'48' b'48' b'24' b'72' b'48' b'48' b'48' b'72' b'72'\n",
            " b'72' b'48' b'72' b'48' b'48' b'72' b'72' b'48']\n",
            "A batch of cp_dose: [b'D1' b'D1' b'D1' b'D1' b'D2' b'D1' b'D2' b'D1' b'D1' b'D2' b'D2' b'D2'\n",
            " b'D1' b'D2' b'D1' b'D1' b'D1' b'D2' b'D2' b'D1' b'D2' b'D1' b'D1' b'D1'\n",
            " b'D2' b'D1' b'D1' b'D2' b'D1' b'D1' b'D1' b'D2']\n",
            "A batch of targets: [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdstNs2QuXxr"
      },
      "source": [
        "# Feature Engineering\n",
        "\n",
        "As we can see in the dataframe we have two types of features:\n",
        "\n",
        "**categorical features** represented by : \n",
        "\n",
        "\n",
        "*   cp_type => trt_cp or ctl_vehicle\n",
        "*   cp_dose => D1 or D2\n",
        "*   cp_time => 24 , 48 , 72\n",
        "\n",
        "\n",
        "**numerical features** represented by :\n",
        "\n",
        "*   g-n  (n E [0 , 771])\n",
        "*   c-m (m E [0 , 99]) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5bbI7hNu_yx"
      },
      "source": [
        "First we will map string values from a vocabulary to integer indices and one-hot encodes the features.\n",
        "\n",
        "Secondly we will **not** normalize the numerical features because they are already very small values and not scattered."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRu5Z_Phuzrx"
      },
      "source": [
        "def get_category_encoding_layer(name, dataset, dtype, max_tokens=None):\n",
        "  # Create a StringLookup layer which will turn strings into integer indices\n",
        "  if dtype == 'string':\n",
        "    index = preprocessing.StringLookup(max_tokens=max_tokens)\n",
        "  else:\n",
        "    index = preprocessing.IntegerLookup(max_values=max_tokens)\n",
        "\n",
        "  # Prepare a Dataset that only yields our feature\n",
        "  feature_ds = dataset.map(lambda x, y: x[name])\n",
        "\n",
        "  # Learn the set of possible values and assign them a fixed integer index.\n",
        "  index.adapt(feature_ds)\n",
        "\n",
        "  # Create a Discretization for our integer indices.\n",
        "  encoder = preprocessing.CategoryEncoding(max_tokens=index.vocab_size())\n",
        "\n",
        "  # Prepare a Dataset that only yields our feature.\n",
        "  feature_ds = feature_ds.map(index)\n",
        "\n",
        "  # Learn the space of possible indices.\n",
        "  encoder.adapt(feature_ds)\n",
        "\n",
        "  # Apply one-hot encoding to our indices. The lambda function captures the\n",
        "  # layer so we can use them, or include them in the functional model later.\n",
        "  return lambda feature: encoder(index(feature))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAS6-c2P1Wgr"
      },
      "source": [
        "all_inputs = []\n",
        "encoded_features = []"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYqyoiPlvz1W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8f3b020-e423-4d34-9e0b-9ca348ebb00c"
      },
      "source": [
        "# Categorical features encoded as string.\n",
        "categorical_cols = ['cp_dose','cp_type','cp_time']\n",
        "for header in categorical_cols:\n",
        "  print(f\"Encoding {header}\")\n",
        "  categorical_col = tf.keras.Input(shape=(1,), name=header, dtype='string')\n",
        "  encoding_layer = get_category_encoding_layer(header, train, dtype='string',\n",
        "                                               max_tokens=5)\n",
        "  encoded_categorical_col = encoding_layer(categorical_col)\n",
        "  all_inputs.append(categorical_col)\n",
        "  encoded_features.append(encoded_categorical_col)\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoding cp_dose\n",
            "Encoding cp_type\n",
            "Encoding cp_time\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wv1UOlY-S5ct"
      },
      "source": [
        "to_skip = ['sig_id', 'cp_type', 'cp_time', 'cp_dose']\n",
        "\n",
        "for header in cols_features:\n",
        "  if header not in to_skip:\n",
        "    categorical_col = tf.keras.Input(shape=(1,), name=header, dtype='float32')\n",
        "    all_inputs.append(categorical_col)\n",
        "    encoded_features.append(categorical_col)\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyYWADKZFi_b"
      },
      "source": [
        "HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([256, 512, 1024]))\n",
        "HP_DROPOUT = hp.HParam('dropout', hp.Discrete([0.1, 0.2, 0.5]))\n",
        "\n",
        "METRIC_ACCURACY = 'accuracy'\n",
        "\n",
        "with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
        "  hp.hparams_config(\n",
        "    hparams=[HP_NUM_UNITS, HP_DROPOUT],\n",
        "    metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],\n",
        "  )"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mW2SgaWXSbG"
      },
      "source": [
        "def train_test_model(hparams):\n",
        "  all_features = tf.keras.layers.concatenate(encoded_features)\n",
        "  x = tf.keras.layers.Dense(875, activation=\"relu\")(all_features)\n",
        "  x = tf.keras.layers.Dense(hparams[HP_NUM_UNITS], activation=\"relu\")(x)\n",
        "  x = tf.keras.layers.Dropout(hparams[HP_DROPOUT])(x)\n",
        "  output = tf.keras.layers.Dense(206)(x)\n",
        "  model = tf.keras.Model(all_inputs, output)\n",
        "  model.compile(optimizer='adam',\n",
        "                loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "                metrics=[\"accuracy\"])\n",
        "  \n",
        "  Model_hist = model.fit(train, epochs=10, validation_data=val)\n",
        "  return Model_hist"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xl3f7SVbGWu6"
      },
      "source": [
        "def run(hparams):\n",
        "    hp.hparams(hparams)  # record the values used in this trial\n",
        "    hist.append(train_test_model(hparams))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32v0t6gYHgba",
        "outputId": "e55bade0-c32c-4ecd-b4a2-636f99237c12"
      },
      "source": [
        "session_num = 0\n",
        "hist = []\n",
        "\n",
        "for num_units in HP_NUM_UNITS.domain.values:\n",
        "  for dropout_rate in HP_DROPOUT.domain.values:\n",
        "    hparams = {\n",
        "        HP_NUM_UNITS: num_units,\n",
        "        HP_DROPOUT: dropout_rate\n",
        "    }\n",
        "    run_name = \"run-%d\" % session_num\n",
        "    print('--- Starting trial: %s' % run_name)\n",
        "    print({h.name: hparams[h] for h in hparams})\n",
        "    run(hparams)\n",
        "    session_num += 1"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- Starting trial: run-0\n",
            "{'num_units': 256, 'dropout': 0.1}\n",
            "Epoch 1/10\n",
            "521/521 [==============================] - 57s 110ms/step - loss: 0.0276 - accuracy: 0.0915 - val_loss: 0.0158 - val_accuracy: 0.1372\n",
            "Epoch 2/10\n",
            "521/521 [==============================] - 59s 113ms/step - loss: 0.0155 - accuracy: 0.1501 - val_loss: 0.0130 - val_accuracy: 0.1968\n",
            "Epoch 3/10\n",
            "521/521 [==============================] - 64s 122ms/step - loss: 0.0125 - accuracy: 0.2161 - val_loss: 0.0102 - val_accuracy: 0.2742\n",
            "Epoch 4/10\n",
            "521/521 [==============================] - 60s 116ms/step - loss: 0.0095 - accuracy: 0.2979 - val_loss: 0.0073 - val_accuracy: 0.3930\n",
            "Epoch 5/10\n",
            "521/521 [==============================] - 63s 120ms/step - loss: 0.0066 - accuracy: 0.4035 - val_loss: 0.0048 - val_accuracy: 0.4572\n",
            "Epoch 6/10\n",
            "521/521 [==============================] - 62s 119ms/step - loss: 0.0042 - accuracy: 0.4889 - val_loss: 0.0030 - val_accuracy: 0.5093\n",
            "Epoch 7/10\n",
            "521/521 [==============================] - 63s 121ms/step - loss: 0.0029 - accuracy: 0.5226 - val_loss: 0.0023 - val_accuracy: 0.5173\n",
            "Epoch 8/10\n",
            "521/521 [==============================] - 64s 123ms/step - loss: 0.0024 - accuracy: 0.5351 - val_loss: 0.0019 - val_accuracy: 0.5229\n",
            "Epoch 9/10\n",
            "521/521 [==============================] - 64s 123ms/step - loss: 0.0019 - accuracy: 0.5382 - val_loss: 0.0015 - val_accuracy: 0.5278\n",
            "Epoch 10/10\n",
            "521/521 [==============================] - 65s 124ms/step - loss: 0.0017 - accuracy: 0.5421 - val_loss: 0.0017 - val_accuracy: 0.5288\n",
            "--- Starting trial: run-1\n",
            "{'num_units': 256, 'dropout': 0.2}\n",
            "Epoch 1/10\n",
            "521/521 [==============================] - 69s 132ms/step - loss: 0.0283 - accuracy: 0.0843 - val_loss: 0.0157 - val_accuracy: 0.1324\n",
            "Epoch 2/10\n",
            "521/521 [==============================] - 74s 141ms/step - loss: 0.0159 - accuracy: 0.1356 - val_loss: 0.0130 - val_accuracy: 0.1940\n",
            "Epoch 3/10\n",
            "521/521 [==============================] - 63s 120ms/step - loss: 0.0133 - accuracy: 0.1920 - val_loss: 0.0108 - val_accuracy: 0.2579\n",
            "Epoch 4/10\n",
            "521/521 [==============================] - 63s 121ms/step - loss: 0.0107 - accuracy: 0.2622 - val_loss: 0.0079 - val_accuracy: 0.3507\n",
            "Epoch 5/10\n",
            "521/521 [==============================] - 62s 118ms/step - loss: 0.0080 - accuracy: 0.3484 - val_loss: 0.0058 - val_accuracy: 0.4311\n",
            "Epoch 6/10\n",
            "521/521 [==============================] - 72s 138ms/step - loss: 0.0056 - accuracy: 0.4377 - val_loss: 0.0040 - val_accuracy: 0.4862\n",
            "Epoch 7/10\n",
            "521/521 [==============================] - 60s 114ms/step - loss: 0.0039 - accuracy: 0.4877 - val_loss: 0.0027 - val_accuracy: 0.5136\n",
            "Epoch 8/10\n",
            "521/521 [==============================] - 59s 114ms/step - loss: 0.0030 - accuracy: 0.5134 - val_loss: 0.0019 - val_accuracy: 0.5267\n",
            "Epoch 9/10\n",
            "521/521 [==============================] - 58s 112ms/step - loss: 0.0025 - accuracy: 0.5239 - val_loss: 0.0017 - val_accuracy: 0.5279\n",
            "Epoch 10/10\n",
            "521/521 [==============================] - 61s 117ms/step - loss: 0.0021 - accuracy: 0.5310 - val_loss: 0.0016 - val_accuracy: 0.5290\n",
            "--- Starting trial: run-2\n",
            "{'num_units': 256, 'dropout': 0.5}\n",
            "Epoch 1/10\n",
            "521/521 [==============================] - 58s 112ms/step - loss: 0.0341 - accuracy: 0.0686 - val_loss: 0.0168 - val_accuracy: 0.1576\n",
            "Epoch 2/10\n",
            "521/521 [==============================] - 57s 109ms/step - loss: 0.0178 - accuracy: 0.1179 - val_loss: 0.0145 - val_accuracy: 0.1610\n",
            "Epoch 3/10\n",
            "521/521 [==============================] - 57s 109ms/step - loss: 0.0158 - accuracy: 0.1407 - val_loss: 0.0133 - val_accuracy: 0.1859\n",
            "Epoch 4/10\n",
            "521/521 [==============================] - 59s 113ms/step - loss: 0.0145 - accuracy: 0.1639 - val_loss: 0.0121 - val_accuracy: 0.2244\n",
            "Epoch 5/10\n",
            "521/521 [==============================] - 62s 119ms/step - loss: 0.0132 - accuracy: 0.1940 - val_loss: 0.0106 - val_accuracy: 0.2757\n",
            "Epoch 6/10\n",
            "521/521 [==============================] - 57s 110ms/step - loss: 0.0117 - accuracy: 0.2313 - val_loss: 0.0095 - val_accuracy: 0.3243\n",
            "Epoch 7/10\n",
            "521/521 [==============================] - 59s 114ms/step - loss: 0.0102 - accuracy: 0.2745 - val_loss: 0.0073 - val_accuracy: 0.3947\n",
            "Epoch 8/10\n",
            "521/521 [==============================] - 58s 112ms/step - loss: 0.0089 - accuracy: 0.3181 - val_loss: 0.0058 - val_accuracy: 0.4386\n",
            "Epoch 9/10\n",
            "521/521 [==============================] - 61s 117ms/step - loss: 0.0075 - accuracy: 0.3653 - val_loss: 0.0046 - val_accuracy: 0.4801\n",
            "Epoch 10/10\n",
            "521/521 [==============================] - 61s 118ms/step - loss: 0.0063 - accuracy: 0.4033 - val_loss: 0.0035 - val_accuracy: 0.5076\n",
            "--- Starting trial: run-3\n",
            "{'num_units': 512, 'dropout': 0.1}\n",
            "Epoch 1/10\n",
            "521/521 [==============================] - 67s 129ms/step - loss: 0.0253 - accuracy: 0.0954 - val_loss: 0.0157 - val_accuracy: 0.1510\n",
            "Epoch 2/10\n",
            "521/521 [==============================] - 62s 119ms/step - loss: 0.0150 - accuracy: 0.1550 - val_loss: 0.0125 - val_accuracy: 0.1999\n",
            "Epoch 3/10\n",
            "521/521 [==============================] - 61s 118ms/step - loss: 0.0115 - accuracy: 0.2353 - val_loss: 0.0091 - val_accuracy: 0.3106\n",
            "Epoch 4/10\n",
            "521/521 [==============================] - 63s 121ms/step - loss: 0.0079 - accuracy: 0.3546 - val_loss: 0.0066 - val_accuracy: 0.4153\n",
            "Epoch 5/10\n",
            "521/521 [==============================] - 66s 126ms/step - loss: 0.0047 - accuracy: 0.4732 - val_loss: 0.0037 - val_accuracy: 0.4864\n",
            "Epoch 6/10\n",
            "521/521 [==============================] - 64s 123ms/step - loss: 0.0028 - accuracy: 0.5208 - val_loss: 0.0028 - val_accuracy: 0.5069\n",
            "Epoch 7/10\n",
            "521/521 [==============================] - 62s 118ms/step - loss: 0.0022 - accuracy: 0.5313 - val_loss: 0.0026 - val_accuracy: 0.5146\n",
            "Epoch 8/10\n",
            "521/521 [==============================] - 63s 122ms/step - loss: 0.0019 - accuracy: 0.5363 - val_loss: 0.0019 - val_accuracy: 0.5253\n",
            "Epoch 9/10\n",
            "521/521 [==============================] - 63s 121ms/step - loss: 0.0016 - accuracy: 0.5358 - val_loss: 0.0015 - val_accuracy: 0.5274\n",
            "Epoch 10/10\n",
            "521/521 [==============================] - 66s 126ms/step - loss: 0.0016 - accuracy: 0.5420 - val_loss: 0.0014 - val_accuracy: 0.5288\n",
            "--- Starting trial: run-4\n",
            "{'num_units': 512, 'dropout': 0.2}\n",
            "Epoch 1/10\n",
            "521/521 [==============================] - 70s 135ms/step - loss: 0.0257 - accuracy: 0.0896 - val_loss: 0.0158 - val_accuracy: 0.1388\n",
            "Epoch 2/10\n",
            "521/521 [==============================] - 69s 133ms/step - loss: 0.0153 - accuracy: 0.1486 - val_loss: 0.0126 - val_accuracy: 0.1952\n",
            "Epoch 3/10\n",
            "521/521 [==============================] - 70s 134ms/step - loss: 0.0121 - accuracy: 0.2199 - val_loss: 0.0096 - val_accuracy: 0.2959\n",
            "Epoch 4/10\n",
            "521/521 [==============================] - 72s 139ms/step - loss: 0.0088 - accuracy: 0.3197 - val_loss: 0.0065 - val_accuracy: 0.4045\n",
            "Epoch 5/10\n",
            "521/521 [==============================] - 64s 124ms/step - loss: 0.0058 - accuracy: 0.4289 - val_loss: 0.0042 - val_accuracy: 0.4710\n",
            "Epoch 6/10\n",
            "521/521 [==============================] - 61s 118ms/step - loss: 0.0036 - accuracy: 0.4965 - val_loss: 0.0028 - val_accuracy: 0.5092\n",
            "Epoch 7/10\n",
            "521/521 [==============================] - 69s 133ms/step - loss: 0.0027 - accuracy: 0.5280 - val_loss: 0.0023 - val_accuracy: 0.5150\n",
            "Epoch 8/10\n",
            "521/521 [==============================] - 71s 136ms/step - loss: 0.0023 - accuracy: 0.5334 - val_loss: 0.0017 - val_accuracy: 0.5257\n",
            "Epoch 9/10\n",
            "521/521 [==============================] - 66s 127ms/step - loss: 0.0019 - accuracy: 0.5336 - val_loss: 0.0015 - val_accuracy: 0.5296\n",
            "Epoch 10/10\n",
            "521/521 [==============================] - 63s 121ms/step - loss: 0.0016 - accuracy: 0.5409 - val_loss: 0.0014 - val_accuracy: 0.5341\n",
            "--- Starting trial: run-5\n",
            "{'num_units': 512, 'dropout': 0.5}\n",
            "Epoch 1/10\n",
            "521/521 [==============================] - 68s 131ms/step - loss: 0.0288 - accuracy: 0.0780 - val_loss: 0.0166 - val_accuracy: 0.1321\n",
            "Epoch 2/10\n",
            "521/521 [==============================] - 78s 150ms/step - loss: 0.0169 - accuracy: 0.1255 - val_loss: 0.0141 - val_accuracy: 0.1682\n",
            "Epoch 3/10\n",
            "521/521 [==============================] - 82s 158ms/step - loss: 0.0147 - accuracy: 0.1606 - val_loss: 0.0120 - val_accuracy: 0.2199\n",
            "Epoch 4/10\n",
            "521/521 [==============================] - 85s 164ms/step - loss: 0.0128 - accuracy: 0.1994 - val_loss: 0.0100 - val_accuracy: 0.2840\n",
            "Epoch 5/10\n",
            "521/521 [==============================] - 79s 151ms/step - loss: 0.0108 - accuracy: 0.2553 - val_loss: 0.0083 - val_accuracy: 0.3587\n",
            "Epoch 6/10\n",
            "521/521 [==============================] - 84s 160ms/step - loss: 0.0088 - accuracy: 0.3193 - val_loss: 0.0063 - val_accuracy: 0.4297\n",
            "Epoch 7/10\n",
            "521/521 [==============================] - 85s 162ms/step - loss: 0.0069 - accuracy: 0.3877 - val_loss: 0.0041 - val_accuracy: 0.4827\n",
            "Epoch 8/10\n",
            "521/521 [==============================] - 79s 152ms/step - loss: 0.0054 - accuracy: 0.4394 - val_loss: 0.0030 - val_accuracy: 0.5143\n",
            "Epoch 9/10\n",
            "521/521 [==============================] - 84s 160ms/step - loss: 0.0044 - accuracy: 0.4716 - val_loss: 0.0024 - val_accuracy: 0.5206\n",
            "Epoch 10/10\n",
            "521/521 [==============================] - 83s 160ms/step - loss: 0.0039 - accuracy: 0.4921 - val_loss: 0.0019 - val_accuracy: 0.5243\n",
            "--- Starting trial: run-6\n",
            "{'num_units': 1024, 'dropout': 0.1}\n",
            "Epoch 1/10\n",
            "521/521 [==============================] - 85s 162ms/step - loss: 0.0239 - accuracy: 0.0971 - val_loss: 0.0156 - val_accuracy: 0.1629\n",
            "Epoch 2/10\n",
            "521/521 [==============================] - 84s 161ms/step - loss: 0.0147 - accuracy: 0.1610 - val_loss: 0.0129 - val_accuracy: 0.2118\n",
            "Epoch 3/10\n",
            "521/521 [==============================] - 84s 162ms/step - loss: 0.0109 - accuracy: 0.2517 - val_loss: 0.0087 - val_accuracy: 0.3150\n",
            "Epoch 4/10\n",
            "521/521 [==============================] - 87s 167ms/step - loss: 0.0066 - accuracy: 0.3970 - val_loss: 0.0052 - val_accuracy: 0.4358\n",
            "Epoch 5/10\n",
            "521/521 [==============================] - 92s 177ms/step - loss: 0.0035 - accuracy: 0.5026 - val_loss: 0.0033 - val_accuracy: 0.4900\n",
            "Epoch 6/10\n",
            "521/521 [==============================] - 87s 166ms/step - loss: 0.0025 - accuracy: 0.5262 - val_loss: 0.0028 - val_accuracy: 0.5073\n",
            "Epoch 7/10\n",
            "521/521 [==============================] - 84s 161ms/step - loss: 0.0020 - accuracy: 0.5367 - val_loss: 0.0021 - val_accuracy: 0.5169\n",
            "Epoch 8/10\n",
            "521/521 [==============================] - 93s 179ms/step - loss: 0.0016 - accuracy: 0.5481 - val_loss: 0.0019 - val_accuracy: 0.5381\n",
            "Epoch 9/10\n",
            "521/521 [==============================] - 92s 176ms/step - loss: 0.0014 - accuracy: 0.5574 - val_loss: 0.0020 - val_accuracy: 0.5243\n",
            "Epoch 10/10\n",
            "521/521 [==============================] - 90s 173ms/step - loss: 0.0013 - accuracy: 0.5382 - val_loss: 0.0012 - val_accuracy: 0.5381\n",
            "--- Starting trial: run-7\n",
            "{'num_units': 1024, 'dropout': 0.2}\n",
            "Epoch 1/10\n",
            "521/521 [==============================] - 79s 152ms/step - loss: 0.0242 - accuracy: 0.0962 - val_loss: 0.0155 - val_accuracy: 0.1545\n",
            "Epoch 2/10\n",
            "521/521 [==============================] - 80s 153ms/step - loss: 0.0150 - accuracy: 0.1535 - val_loss: 0.0124 - val_accuracy: 0.2115\n",
            "Epoch 3/10\n",
            "521/521 [==============================] - 75s 144ms/step - loss: 0.0114 - accuracy: 0.2367 - val_loss: 0.0093 - val_accuracy: 0.3188\n",
            "Epoch 4/10\n",
            "521/521 [==============================] - 69s 131ms/step - loss: 0.0075 - accuracy: 0.3657 - val_loss: 0.0053 - val_accuracy: 0.4351\n",
            "Epoch 5/10\n",
            "521/521 [==============================] - 81s 155ms/step - loss: 0.0042 - accuracy: 0.4799 - val_loss: 0.0035 - val_accuracy: 0.4866\n",
            "Epoch 6/10\n",
            "521/521 [==============================] - 77s 147ms/step - loss: 0.0029 - accuracy: 0.5194 - val_loss: 0.0031 - val_accuracy: 0.5019\n",
            "Epoch 7/10\n",
            "521/521 [==============================] - 67s 129ms/step - loss: 0.0023 - accuracy: 0.5260 - val_loss: 0.0021 - val_accuracy: 0.5150\n",
            "Epoch 8/10\n",
            "521/521 [==============================] - 67s 129ms/step - loss: 0.0019 - accuracy: 0.5360 - val_loss: 0.0017 - val_accuracy: 0.5233\n",
            "Epoch 9/10\n",
            "521/521 [==============================] - 66s 127ms/step - loss: 0.0016 - accuracy: 0.5440 - val_loss: 0.0017 - val_accuracy: 0.5253\n",
            "Epoch 10/10\n",
            "521/521 [==============================] - 64s 123ms/step - loss: 0.0015 - accuracy: 0.5422 - val_loss: 0.0014 - val_accuracy: 0.5271\n",
            "--- Starting trial: run-8\n",
            "{'num_units': 1024, 'dropout': 0.5}\n",
            "Epoch 1/10\n",
            "521/521 [==============================] - 63s 121ms/step - loss: 0.0257 - accuracy: 0.0855 - val_loss: 0.0162 - val_accuracy: 0.1465\n",
            "Epoch 2/10\n",
            "521/521 [==============================] - 65s 124ms/step - loss: 0.0163 - accuracy: 0.1325 - val_loss: 0.0136 - val_accuracy: 0.1832\n",
            "Epoch 3/10\n",
            "521/521 [==============================] - 65s 124ms/step - loss: 0.0137 - accuracy: 0.1768 - val_loss: 0.0111 - val_accuracy: 0.2470\n",
            "Epoch 4/10\n",
            "521/521 [==============================] - 64s 124ms/step - loss: 0.0113 - accuracy: 0.2351 - val_loss: 0.0087 - val_accuracy: 0.3282\n",
            "Epoch 5/10\n",
            "521/521 [==============================] - 65s 126ms/step - loss: 0.0086 - accuracy: 0.3208 - val_loss: 0.0062 - val_accuracy: 0.4287\n",
            "Epoch 6/10\n",
            "521/521 [==============================] - 67s 128ms/step - loss: 0.0063 - accuracy: 0.4076 - val_loss: 0.0038 - val_accuracy: 0.4792\n",
            "Epoch 7/10\n",
            "521/521 [==============================] - 66s 127ms/step - loss: 0.0046 - accuracy: 0.4638 - val_loss: 0.0027 - val_accuracy: 0.5087\n",
            "Epoch 8/10\n",
            "521/521 [==============================] - 66s 126ms/step - loss: 0.0038 - accuracy: 0.4911 - val_loss: 0.0024 - val_accuracy: 0.5183\n",
            "Epoch 9/10\n",
            "521/521 [==============================] - 67s 128ms/step - loss: 0.0032 - accuracy: 0.5019 - val_loss: 0.0018 - val_accuracy: 0.5251\n",
            "Epoch 10/10\n",
            "521/521 [==============================] - 68s 131ms/step - loss: 0.0028 - accuracy: 0.5100 - val_loss: 0.0014 - val_accuracy: 0.5306\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdaspPVt5Z12"
      },
      "source": [
        "# Model Variance & Bias Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jd7CjNfmlWyL"
      },
      "source": [
        "We chose to plot insights relative to the seventh model because it is the one with the highest accuracy on the validation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "38JSQLPUxdYn",
        "outputId": "3fe6de4f-202b-4216-db01-ebcc166480d4"
      },
      "source": [
        "loss_train = hist[6].history['loss']\n",
        "loss_validation = hist[6].history['val_loss']\n",
        "epochs = range(1, len(hist[6].history['loss']) + 1)\n",
        "plt.plot(epochs, loss_train, 'g', label='Training')\n",
        "plt.plot(epochs, loss_validation, 'b', label='Validation')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "print(f\"Final validation loss : {hist[6].history['val_loss'][9]}\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3hUddr/8fedAgkhRCDUhLo0QSBAiAUVEAUUBFQUERHUlRWJrrr7uOpjW7Y86qO7Pq5iAVZQUWCxgIL0ZvspRRRpu4hRehMhlEBCvr8/zgRCDJBJZjKT5PO6rrkyc+acM/fMdcnH7yn315xziIiIFFVEqAsQEZGyRcEhIiJ+UXCIiIhfFBwiIuIXBYeIiPhFwSEiIn5RcIiIiF8UHCIBZGYZZnZ5qOsQCSYFh4iI+EXBIRJkZlbZzJ4zs22+x3NmVtn3XqKZfWhmP5vZT2b2sZlF+N77g5ltNbNMM9tgZj1C+01EPFGhLkCkAvhv4AIgBXDAdOAR4FHgd8AWoJZv3QsAZ2YtgXSgs3Num5k1BiJLt2yRwmnEIRJ8Q4DRzrldzrndwB+Bob73soF6QCPnXLZz7mPnNZA7DlQGWptZtHMuwzn3XUiqFylAwSESfPWBH/K9/sG3DOB/gY3AXDPbZGYPAjjnNgL3Ak8Au8xsspnVRyQMKDhEgm8b0Cjf64a+ZTjnMp1zv3PONQX6Affnnctwzr3lnLvYt60DnirdskUKp+AQCbxoM4vJewBvA4+YWS0zSwQeA94EMLO+ZtbMzAzYj3eIKtfMWprZZb6T6FnAESA3NF9H5FQKDpHAm4X3D33eIwZYDnwDrAZWAn/2rdscmA8cBD4HxjjnFuGd33gS2APsAGoDD5XeVxA5PdNETiIi4g+NOERExC9BDQ4z6+27cWlj3tUiBd6vbGZTfO9/4btWHTO7wsxWmNlq39/L8m2z2LfPVb5H7WB+BxEROVXQbgA0s0jgReAKvBuclpnZDOfc2nyr3Q7sc841M7Mb8a4aGYR3XPdq341P5wFzgKR82w1xzi0PVu0iInJ6wRxxpAEbnXObnHPHgMlA/wLr9Acm+p5PA3qYmTnnvnLObfMtXwPE5rVoEBGR0Apmy5EkYHO+11uA80+3jnMux8z2AzXxRhx5rgNWOueO5lv2mpkdB94B/uwKOcNvZiOAEQBxcXGdWrVqVcKvIyJSsaxYsWKPc65WweVh3avKzNrgHb7qmW/xEOfcVjOLxwuOocDrBbd1zr0KvAqQmprqli/XkS0REX+Y2Q+FLQ/moaqtQIN8r5N9ywpdx8yigARgr+91MvAecEv+Hj3Oua2+v5nAW3iHxEREpJQEMziWAc3NrImZVQJuBGYUWGcGMMz3fCCw0DnnzOwcYCbwoHPu07yVzSzKd+ctZhYN9AW+DeJ3EBGRAoIWHM65HLy20HOAdcBU59waMxttZv18q40HaprZRuB+IO+S3XSgGfBYgctuKwNzzOwbYBXeiGVssL6DiIj8UoW4c1znOETKj+zsbLZs2UJWVlaoSyk3YmJiSE5OJjo6+pTlZrbCOZdacP2wPjkuIlLQli1biI+Pp3Hjxni9IaUknHPs3buXLVu20KRJkyJto5YjIlKmZGVlUbNmTYVGgJgZNWvW9GsEp+AQkTJHoRFY/v6eCo7TyHW5jF0xlmlrp4W6FBGRsKLgOI0Ii2DsyrE8tugxKsIFBCJSNHv37iUlJYWUlBTq1q1LUlLSidfHjh0747bLly/nnnvuOetnXHTRRYEqNyh0cvwM0tPSGfb+MBZlLOKyJpedfQMRKfdq1qzJqlWrAHjiiSeoWrUqv//970+8n5OTQ1RU4f+0pqamkpr6i4uUfuGzzz4LTLFBohHHGdzQ5gYSqyTywpcvhLoUEQljw4cP58477+T888/ngQce4Msvv+TCCy+kQ4cOXHTRRWzYsAGAxYsX07dvX8ALndtuu41u3brRtGlTnn/++RP7q1q16on1u3XrxsCBA2nVqhVDhgw5cQRk1qxZtGrVik6dOnHPPfec2G9p0IjjDGKiYrij4x089elT/Lj/RxomNAx1SSKSz72z72XVjlUB3WdK3RSe6/2c39tt2bKFzz77jMjISA4cOMDHH39MVFQU8+fP5+GHH+add975xTbr169n0aJFZGZm0rJlS0aOHPmLeym++uor1qxZQ/369enSpQuffvopqamp/OY3v2Hp0qU0adKEwYMHF/v7FodGHGdxZ+qdALy8/OUQVyIi4ez6668nMjISgP3793P99ddz3nnncd9997FmzZpCt+nTpw+VK1cmMTGR2rVrs3Pnzl+sk5aWRnJyMhEREaSkpJCRkcH69etp2rTpifsuSjs4NOI4i4YJDenfsr93orzrY8RExYS6JBHxKc7IIFji4uJOPH/00Ufp3r077733HhkZGXTr1q3QbSpXPjnNUGRkJDk5OcVap7RpxFEE6Wnp7Dm8h6lrpoa6FBEpA/bv309Skjdp6YQJEwK+/5YtW7Jp0yYyMjIAmDJlSsA/40wUHEXQvXF3zk08VyfJRaRIHnjgAR566CE6dOgQlBFCbGwsY8aMoXfv3nTq1In4+HgSEhIC/jmnoyaHRTRm2RhGzRrFF7/+grQkTQEiEirr1q3j3HPPDXUZIXfw4EGqVq2Kc45Ro0bRvHlz7rvvvmLvr7Df9XRNDjXiKKKh7YYSXyleow4RCQtjx44lJSWFNm3asH//fn7zm9+U2mcrOIoovnI8w1OGM2XNFHYd2hXqckSkgrvvvvtYtWoVa9euZdKkSVSpUqXUPlvB4Ye7Ot/FsePHGLdyXKhLEREJGQWHH1oltuKKplfw0vKXyMkN/SVxIiKhoODwU3paOlsObGHGhoLTp4uIVAwKDj/1ad6HRgmNdJJcRCosBYefIiMiuavzXSzKWMSaXYW3ERCR8qt79+7MmTPnlGXPPfccI0eOLHT9bt26kXc7wFVXXcXPP//8i3WeeOIJnnnmmTN+7vvvv8/atWtPvH7ssceYP3++v+UHhIKjGG7vcDsxUTG8uOzFUJciIqVs8ODBTJ48+ZRlkydPLlK/qFmzZnHOOecU63MLBsfo0aO5/PLLi7WvklJwFEPNKjUZfN5gXv/6dfZn7Q91OSJSigYOHMjMmTNPTNqUkZHBtm3bePvtt0lNTaVNmzY8/vjjhW7buHFj9uzZA8Bf/vIXWrRowcUXX3yi7Tp492d07tyZ9u3bc91113H48GE+++wzZsyYwX/913+RkpLCd999x/Dhw5k2zZuhdMGCBXTo0IG2bdty2223cfTo0ROf9/jjj9OxY0fatm3L+vXrA/IbqMlhMaWnpfPaqteY+PVE7jn/7DN6iUjg3XsvrApsV3VSUuC5M/ROrFGjBmlpaXz00Uf079+fyZMnc8MNN/Dwww9To0YNjh8/To8ePfjmm29o165doftYsWIFkydPZtWqVeTk5NCxY0c6deoEwLXXXssdd9wBwCOPPML48eO5++676devH3379mXgwIGn7CsrK4vhw4ezYMECWrRowS233MJLL73EvffeC0BiYiIrV65kzJgxPPPMM4wbV/LbCTTiKKaO9TpyYfKFvLjsRXJdbqjLEZFSlP9wVd5hqqlTp9KxY0c6dOjAmjVrTjmsVNDHH3/MNddcQ5UqVahWrRr9+vU78d63337LJZdcQtu2bZk0adJpW7Ln2bBhA02aNKFFixYADBs2jKVLl554/9prrwWgU6dOJ5oilpRGHCWQnpbOkHeHMH/TfHr+qmeoyxGpcM40Mgim/v37c99997Fy5UoOHz5MjRo1eOaZZ1i2bBnVq1dn+PDhZGVlFWvfw4cP5/3336d9+/ZMmDCBxYsXl6jWvLbsgWzJrhFHCQxsPZA6cXV0aa5IBVO1alW6d+/ObbfdxuDBgzlw4ABxcXEkJCSwc+dOPvroozNuf+mll/L+++9z5MgRMjMz+eCDD068l5mZSb169cjOzmbSpEknlsfHx5OZmfmLfbVs2ZKMjAw2btwIwBtvvEHXrl0D9E0Lp+AogUqRlRjRaQQf/vtDvt/3fajLEZFSNHjwYL7++msGDx5M+/bt6dChA61ateKmm26iS5cuZ9y2Y8eODBo0iPbt23PllVfSuXPnE+/96U9/4vzzz6dLly60atXqxPIbb7yR//3f/6VDhw589913J5bHxMTw2muvcf3119O2bVsiIiK48847A/+F81Fb9RLaemArjZ5rxP0X3s/TVzwdlM8QkZPUVj041Fa9FCVVS+Lac69l3MpxHM4+HOpyRESCTsERAOlp6ezL2sfkbyeffWURkTJOwREAlzS8hLa12/KPL/9BRTj0JxJq+u8ssPz9PRUcAWBmpKels2rHKj7f8nmoyxEp12JiYti7d6/CI0Ccc+zdu5eYmJgib6P7OAJkSNshPDDvAV748gUuanBRqMsRKbeSk5PZsmULu3fvDnUp5UZMTAzJyclFXl/BESBxleK4rcNt/OPLf/Bsz2epF18v1CWJlEvR0dE0adIk1GVUaDpUFUB3db6LnNwcxq4cG+pSRESCJqjBYWa9zWyDmW00swcLeb+ymU3xvf+FmTX2Lb/CzFaY2Wrf38vybdPJt3yjmT1vZhbM7+CPZjWacWWzK3l5+ctkH88OdTkiIkERtOAws0jgReBKoDUw2MxaF1jtdmCfc64Z8HfgKd/yPcDVzrm2wDDgjXzbvATcATT3PXoH6zsUR3paOtsPbue99e+FuhQRkaAI5ogjDdjonNvknDsGTAb6F1inPzDR93wa0MPMzDn3lXNum2/5GiDWNzqpB1Rzzv0/511S8TowIIjfwW+9m/WmafWm6l8lIuVWMIMjCdic7/UW37JC13HO5QD7gZoF1rkOWOmcO+pbf8tZ9gmAmY0ws+Vmtrw0r76IsAhGdR7Fxz9+zNc7vi61zxURKS1hfXLczNrgHb76jb/bOudedc6lOudSa9WqFfjizuDWlFuJjYrV1LIiUi4FMzi2Ag3yvU72LSt0HTOLAhKAvb7XycB7wC3Oue/yrZ//YuPC9hly1WOrc3O7m3nzmzfZd2RfqMsREQmoYAbHMqC5mTUxs0rAjcCMAuvMwDv5DTAQWOicc2Z2DjATeNA592neys657cABM7vAdzXVLcD0IH6HYhvVeRRHco7w2qrXQl2KiEhABS04fOcs0oE5wDpgqnNujZmNNrO8eRLHAzXNbCNwP5B3yW460Ax4zMxW+R61fe/dBYwDNgLfAWeeMSVE2tdtzyUNL9HUsiJS7mg+jiCaumYqg6YNYuZNM7mq+VWl/vkiIiWh+ThC4JpW11Cvaj1dmisi5YqCI4iiI6O5M/VOPtr4ERt/2hjqckREAkLBEWQjOo0gOiKaMcvGhLoUEZGAUHAEWd2qdRnYeiD//OqfHDp2KNTliIiUmIKjFKSnpbP/6H4mrZ4U6lJEREpMwVEKLky+kA51O/DCly9o1jIRKfMUHKUgb2rZ1btW8/GPH4e6HBGRElFwlJLB5w2mRmwNXZorImWegqOUxEbHcnuH23l33btsPRB27bVERIpMwVGKRqaOJNfl8sqKV0JdiohIsSk4SlGT6k3o26Ivr6x4haM5R0NdjohIsSg4Sll6Wjq7Du3inXXvhLoUEZFiUXCUssubXk6Lmi10klxEyiwFRynLm1r28y2fs2LbilCXIyLiNwVHCAxrP4y46DhNLSsiZZKCIwQSYhK4pf0tvLX6LfYe3hvqckRE/KLgCJFRnUdx9PhRxn81PtSliIj4RcERIm1qt6F74+6MWTaG47nHQ12OiEiRKThCKD0tnR/2/8DM/8wMdSkiIkWm4Aihfi37kVwtWZfmikiZouAIoaiIKEamjmTepnms37M+1OWIiBSJgiPEft3x11SKrKSpZUWkzFBwhFjtuNoMajOICasmkHk0M9TliIiclYIjDKSnpZN5LJM3vnkj1KWIiJyVgiMMpCWl0bl+Z00tKyJlgoIjTKSnpbNuzzoWZSwKdSkiImek4AgTN7S5gcQqibo0V0TCnoIjTMRExXBHxzuYvmE6P+7/MdTliIicloIjjNyZeicALy9/OcSViIicnoIjjDRMaEj/lv0Zu3IsWTlZoS5HRKRQCo4wk56Wzp7De5i6ZmqoSxERKZSC4wxmz4aNG0v3M7s37s65iefqJLmIhC0Fx2lkZ8PIkZCSAuPGQWndXmFmpKels2zbMr7c+mXpfKiIiB8UHKcRHQ1Ll8L558Mdd8A118Du3aXz2UPbDSW+UrxGHSISloIaHGbW28w2mNlGM3uwkPcrm9kU3/tfmFlj3/KaZrbIzA6a2QsFtlns2+cq36N2sOpv0ADmzYNnn4WPPoK2bWHWrGB92knxleMZnjKcKWumsOvQruB/oIiIH4IWHGYWCbwIXAm0BgabWesCq90O7HPONQP+DjzlW54FPAr8/jS7H+KcS/E9gvova0QE3H8/LFsGtWtDnz5w111w+HAwPxXu6nwXx44fY9zKccH9IBERPwVzxJEGbHTObXLOHQMmA/0LrNMfmOh7Pg3oYWbmnDvknPsEL0DCQrt28OWXXoi89BJ06ADLlwfv81oltuKKplfw0vKXyMnNCd4HiYj4KZjBkQRszvd6i29Zoes453KA/UDNIuz7Nd9hqkfNzAJRbFHExHiHrebPh0OH4MIL4S9/gZwg/buenpbOlgNbmLFhRnA+QESkGMriyfEhzrm2wCW+x9DCVjKzEWa23MyW7w7wWe0ePWD1ahg4EB55BLp2hU2bAvoRAPRp3odGCY10klxEwkowg2Mr0CDf62TfskLXMbMoIAHYe6adOue2+v5mAm/hHRIrbL1XnXOpzrnUWrVqFesLnEn16vD22zBpEqxZA+3bw2uvBfay3ciISO7qfBeLMhbx7a5vA7djEZESCGZwLAOam1kTM6sE3AgUPOYyAxjmez4QWOjOMCGFmUWZWaLveTTQFwjpv6g33QTffAOdOsFtt8F118GePYHb/+0dbicuOo6b3rlJV1iJSFgIWnD4zlmkA3OAdcBU59waMxttZv18q40HaprZRuB+4MQlu2aWAfwNGG5mW3xXZFUG5pjZN8AqvBHL2GB9h6Jq2BAWLICnn4YPP/Qu2509OzD7rlmlJtNvnM7GnzbSfWJ3dhzcEZgdi4gUk1WEGedSU1Pd8mBeApXPqlVw883e4av0dHjqKahSpeT7XZKxhD5v9SG5WjILhy2kfnz9ku9UROQMzGyFcy614PKyeHI8rKWkePd8/Pa38MIL3iGslStLvt+ujbsy++bZbM3cStcJXdm8f/PZNxIRCQIFRxDExsJzz8HcuXDggNe25H/+B44fL9l+L254MfOGzmPXoV10ndCVH37+ITAFi4j4QcERRFdc4V22e8018PDD0K0bfP99yfZ5QfIFzB86n31Z++g6oSub9gXhOmARkTNQcARZjRowZQq8/jp8/bV32e7EiSW7bLdzUmcW3LKAzGOZdJ3QlY0/lXLvdxGp0BQcpcAMhg71LttNSYHhw+GGG2DvGe9YObOO9Tqy8JaFZOVk0XVCVzbs2RCwekVEzkTBUYoaN4ZFi7zzHdOne5ftzp1b/P21r9ueRcMWkZObQ7eJ3Vi7e23AahUROR0FRymLjIQHH4QvvoBzzoFevbwrsI4cKd7+zqt9HouHLQag24RuusNcRIJOwREiHTrAihVw993w/POQmgpffVW8fZ1b61wWD1tMdGQ03SZ04+sdXwe2WBGRfIoUHGYWZ2YRvuctzKyfr+WHlEBsrBcas2fDvn3eZbtPPVW8y3ZbJrZkyfAlVImuwmWvX8bK7QG4eUREpBBFHXEsBWLMLAmYi9eRdkKwiqpoevXyLtu9+mrvMNZll8EPxbhFo1mNZiwZvoT4SvH0eL0Hy7YuC3yxIlLhFTU4zDl3GLgWGOOcux5oE7yyKp6aNWHaNK/D7sqV3sRRb77p/2W7Tao3YcnwJdSIrcHlb1zO55s/D07BIlJhFTk4zOxCYAgw07csMjglVVxm3qW6X3/tXXE1dCjceCP89JN/+2l0TiOWDF9C7bja9HyzJ5/8+ElQ6hWRiqmowXEv8BDwnq/DbVNgUfDKqtiaNoUlS7zZBd991xt9fPGFf/tIrpbMkuFLSIpPovebvVmSsSQ4xYpIheN3d1zfSfKqzrkDwSkp8EqzO26grVgB11/vnTxfutQbifhjx8Ed9Hi9B9/v+54PBn9Aj6Y9glOoiJQ7JeqOa2ZvmVk1M4vDmzhprZn9V6CLlF/q1AkWLvRas/fq5f8UtXWr1mXRsEU0q9GMvm/3Zc7GOcEpVEQqjKIeqmrtG2EMAD4CmnCaub4l8Bo3hjlzICsLevaEHX7O5VQ7rjYLhy2kVWIr+k3ux8x/zzz7RiIip1HU4Ij23bcxAJjhnMsGyv8MUGHkvPNg5kzYvh1694aff/Zv+8QqiSy4ZQFta7flminXMH399OAUKiLlXlGD4xUgA4gDlppZI6DMnOMoLy680DtZvnatd8/H4cP+bV8jtgbzb5lPx3odGfivgbyz9p3gFCoi5VqRgsM597xzLsk5d5Xz/AB0D3JtUoheveCNN+DTT70Ou9nZ/m1/Tsw5zB06l7SkNAZNG8SUb6cEp1ARKbeKenI8wcz+ZmbLfY9n8UYfEgKDBsGYMd6hq9tug9xc/7avVrkas4fM5qIGF3HTuzcx6ZtJwSlURMqloh6q+ieQCdzgexwAXgtWUXJ2d94Jf/6zd3f5fff5f4d5fOV4PhryEV0bdWXoe0OZuGpicAoVkXInqojr/co5d12+1380s1XBKEiK7uGHYc8eb37zxER49FH/to+rFMeHN33IgMkDuHX6reTk5nB7x9uDU6yIlBtFHXEcMbOL816YWRegmDNISKCYwbPPeq1JHnvMO3zlryrRVZgxeAa9mvXi1x/8mpeXvxz4QkWkXCnqiONO4HUzS/C93gcMC05J4o+ICBg/3rs8Nz3dm+P8xhv920dMVAzvD3qfgf8ayMiZI8nJzSE9LT04BYtImVfUq6q+ds61B9oB7ZxzHYDLglqZFFl0NEyZAhdf7I0+Zs/2fx+Voyrzzg3vMKDVAO7+6G7+/vnfA1+oiJQLfs0A6Jw7kK9H1f1BqEeKKTYWPvjAu1Hw2mvhs8/830elyEpMHTiVga0Hcv/c+3n606cDX6iIlHlFPVRVGAtYFRIQCQneaOPii6FPn+I1RYyOjObt694mKiKKP8z/A9nHs/nvS/87OAWLSJlUkjnH1XIkDNWpA/PmFb8pIkBURBRvXPMGN7e7mUcWPcIfF/8Rf7soi0j5dcYRh5llUnhAGBAblIqkxPKaIl56qdcU8ZNPoG5d//YRFRHFhP4TiIqI4oklT5Cdm82fuv8JMw00RSq6MwaHcy6+tAqRwDrvPJg1C3r08JoiLl4M55zj3z4iIyIZ32880RHR/OXjv5B9PJsnL39S4SFSwZXkHIeEuQsu8JoiXn2195gzxzuE5Y8Ii+Dlvi8TFRHF0589TeWoyozuPjo4BYtImVCScxxSBpS0KSJ44fHiVS9yW8pt/GnpnxizrBh3GopIuaERRwUwaJA39ezIkV5TxIkTvRsH/WFmvHL1K+w5sof0WekkVknkhjY3BKdgEQlrGnFUECVtigjeCfPJ102mS8Mu3PzuzSzYtCDwhYpI2FNwVCAPPwz33gvPP++FSHHERscy48YZtExsyYApA1i5fWVgixSRsBfU4DCz3ma2wcw2mtmDhbxf2cym+N7/wswa+5bXNLNFZnbQzF4osE0nM1vt2+Z50yU+RZbXFPGWW4rfFBGgemx1Zg+ZTc3Ymlw56Uo2/rQxsIWKSFgLWnCYWSTwInAl0BoYbGatC6x2O7DPOdcM+DvwlG95FvAo8PtCdv0ScAfQ3PfoHfjqy6+ICBg3zrvKKj0dJk8u3n6SqiUx5+Y55Lpcer7Rk+2Z2wNbqIiErWCOONKAjc65Tc65Y8BkoH+BdfoDeTMITQN6mJk55w455z7BC5ATzKweUM059/+cdyvz68CAIH6HcimvKeIllxS/KSJAy8SWzLxpJrsO7eLKSVeyP2t/YAsVkbAUzOBIAjbne73Ft6zQdZxzOcB+oOZZ9rnlLPsEwMxG5E11u3v3bj9LL/9iY2HGjJI1RQRIS0rj3UHvsmb3GvpP7k9WTtbZNxKRMq3cnhx3zr3qnEt1zqXWqlUr1OWEpbymiElJXlPE1auLt5+ev+rJxAETWfLDEoa8O4TjuccDW6iIhJVgBsdWoEG+18m+ZYWuY2ZRQAKw9yz7TD7LPsUPgWiKCHBT25t4rtdzvLvuXe6aeZeaIoqUY8EMjmVAczNrYmaVgBuBGQXWmcHJmQQHAgvdGf7Fcc5tBw6Y2QW+q6luAaYHvvSKpXFjmDsXsrK8pog7dhRvP7+94Lc82OVBXl35Kk8sfiKQJYpIGAnanePOuRwzSwfmAJHAP51za8xsNLDcOTcDGA+8YWYbgZ/wwgUAM8sAqgGVzGwA0NM5txa4C5iA1533I99DSqhNm5I3RQT4a4+/suvQLkYvHU2dqnW4q/NdAa9VRELLKsIhhdTUVLd8+fJQl1EmzJ0LffvC+ecXrykiQE5uDtdNvY4PNnzAlIFTuL7N9YEvVESCzsxWOOdSCy4vtyfHpXh69vTakpSkKeIprUneu5mF3y8MfKEiEjIKDvmFG27w7iqfOdNripib6/8+8lqTtKjZgv6T+6s1iUg5ouCQQgWiKaJak4iUTwoOOa2HH/ZCoyRNEfNakxzPPU7PN3qy42AxL9kSkbCh4JDTMoNnnil5U8SWiS2ZNWQWuw7tovebvdWaRKSMU3DIGRVsivjKK8Xbj1qTiJQfCg45q7ymiFde6Z37ePzx4p3zUGsSkfJBwSFFEhsL06d7V1mNHg0jRkBOjv/7uantTfy91995d927jJo1Sq1JRMogzTkuRRYV5R22ql/fO1m+Y4c3EvH3JsF7L7iXnQd38uSnT1Inrg5/7P7H4BQsIkGh4BC/mMGf/uSFx6hRXouSDz6AxET/9qPWJCJllw5VSbGMHAnvvANffQVdukBGhn/bmxmvXP0KV7e4mvRZ6fxrzb+CUqeIBJ6CQ4rtmmtg/nzYtdtmkFYAABQASURBVAsuvBBWrfJv+6iIKCYPnMxFDS5SaxKRMkTBISVy8cVeX6voaLj0UliwwL/tq0RX4YPBH9C8RnO1JhEpIxQcUmKtW3tTzzZq5F2yO3myf9tXj63OnJvnUCO2hlqTiJQBCg4JiORk+Phj75DV4MHwt7/5t33+1iS93uyl1iQiYUzBIQFzzjneHB7XXQe/+5338KezbqvEVswaMosdB3eoNYlIGFNwSEDFxHj3dqSne6OOm2+GY8eKvn1aUhrv3uC1JhkwZYBak4iEIQWHBFxkpNdR93/+B95+G666Cg4cKPr2vZr1YkL/CSzOWKzWJCJhSMEhQWEGDz4IEyfCkiXQtSts31707Ye0G6LWJCJhSneOS1DdcgvUrg0DB8JFF3nnQFq0KNq2ak0iEp404pCg690bFi2CQ4e88Pjii6Jv+9cef+XWlFsZvXQ0Y5YVc0IQEQkoBYeUis6dvXs9EhKge3f48MOibWdmvHr1q2pNIhJGFBxSapo188KjdWsYMADGjy/admpNIhJeFBxSqurUgcWL4fLL4de/9jrtFuW8d8HWJB/+u4hDFhEJOAWHlLqqVb1W7EOHenOZjxwJx4twxW312OrMHTqXFjVbcPXbVzN6yWhynR93GIpIQCg4JCSio71Ldf/wB28e84ED4ciRs29XP74+n9z6CUPbDeXxxY9z7ZRrOXDUj5tERKTEFBwSMmbw5JPezYLTp8MVV8BPP519u9joWCYOmMj/9f4/Pvz3h6SNTWP9nvXBL1hEAAWHhIG77/balCxb5rVp//HHs29jZtxz/j0suGUBPx35ibSxaUxfPz34xYqIgkPCw/XXezcHbtvmddhdvbpo23Vt3JUVI1bQMrElA6YM4PFFj+u8h0iQKTgkbHTr5rVmB2/ksXhx0bZrkNCApcOXMqz9MEYvHU3/yf3VWVckiBQcElbatoXPP4ekJOjVC/5VxPv9YqNjea3/a7xw5QvM3jibtHFprN29NrjFilRQCg4JOw0bwiefQGoqDBoE//hH0bYzM0aljWLhLQv5Oetnzh93Pu+tey+4xYpUQAoOCUs1asD8+dC/P9xzj9dpt6gNci9pdAkrRqygda3WXDv1Wh5d+Khas4sEkIJDwlZsLEybBnfeCU89BcOGQXZ20bZNrpbMkuFLuC3lNv788Z/pN7kfP2f9HNyCRSoIBYeEtchIGDPGa03yxhvQty9kZhZt25ioGMb1G8dLfV5i3nfz6Dy2M9/u+ja4BYtUAEENDjPrbWYbzGyjmT1YyPuVzWyK7/0vzKxxvvce8i3fYGa98i3PMLPVZrbKzJYHs34JD2bwyCMwbhwsWOBNCpV39dXZtzXuTL2TRcMWcfDYQS4YdwHT1k4LbsEi5VzQgsPMIoEXgSuB1sBgM2tdYLXbgX3OuWbA34GnfNu2Bm4E2gC9gTG+/eXp7pxLcc6lBqt+CT+33+7dYb51K1x6qXf57oIFRTv30aVhF1aMWEHbOm25/l/X89D8h3TeQ6SYgjniSAM2Ouc2OeeOAZOB/gXW6Q9M9D2fBvQwM/Mtn+ycO+qc+x7Y6NufVHB9+sD338Nzz8F//uN12e3SBT766OwBUj++PouHLeaOjnfw5KdP0uetPvx0pAg9TkTkFMEMjiRgc77XW3zLCl3HOZcD7AdqnmVbB8w1sxVmNuJ0H25mI8xsuZkt3717d4m+iISXKlXgt7+F777zzn9s2QJXXQVpad6I5EwBUjmqMq9e/Sqv9H2Fhd8vpPPYzqzeWcTb1EUEKJsnxy92znXEOwQ2yswuLWwl59yrzrlU51xqrVq1SrdCKRUxMV5L9o0bvfMfP/3kTRCVkuLdOJh7hs4jIzqNYMnwJRzJPsIF4y9g6pqppVe4SBkXzODYCjTI9zrZt6zQdcwsCkgA9p5pW+dc3t9dwHvoEFaFV6mSd/5jwwZ4/XU4ehRuuMG7C/2tt04/18eFDS5kxYgVpNRNYdC0Qfxh3h903kOkCIIZHMuA5mbWxMwq4Z3snlFgnRnAMN/zgcBC55zzLb/Rd9VVE6A58KWZxZlZPICZxQE9AV1fKQBERXmTQ61ZA5MnQ0QEDBkC554LEyYUfg9Ivfh6LBq2iJGpI3n6s6e5ctKV7D28t9RrFylLghYcvnMW6cAcYB0w1Tm3xsxGm1k/32rjgZpmthG4H3jQt+0aYCqwFpgNjHLOHQfqAJ+Y2dfAl8BM59zsYH0HKZsiI71WJV9/De+84804eOut0KIFvPqqNyLJr1JkJcb0GcO4q8ex5IcldB7bma93fB2a4kXKAHNF7eNQhqWmprrly3XLR0XlHMya5d1E+MUXkJzszTz4619750ny+2LLF1w79Vr2HdnH+H7jGdx2cGiKFgkDZraisNseyuLJcRG/mHmX8X7+OcydC02aeJNHNWkCf/sbHDp0ct3zk89nxYgVdKrfiZvevYnfz/09Obk5oSteJAwpOKTCMPOmp1261Jvro3Vr+N3voHFjbwrbvFYmdavWZcEtCxjVeRTPfv4svd7sxZ7De0JZukhYUXBIhdS1q3fX+aefeu3bH3oIGjWC0aPh55+98x4vXPUC/+z3Tz798VNSX01l5faVoS5bJCwoOKRCu+gi767zL7+ESy6Bxx/3AuSRR2DvXri1w618fOvHHHfH6fLPLkz6ZlKoSxYJOQWHCNC5s3fX+apV0LMn/PWvXoA88AA0jOrMihErSEtK4+b3bua+2ffpvIdUaAoOkXzat/fuOv/2W28SqWef9U6i//W/a/N6j/nck3YPz33xHFe8cQW7D6mVjVRMCg6RQrRuDZMmwfr13j0hL7wALZpFk/Ph//G386fx+ebPafaPZlw75VpeWvYS/9n7HyrCpe0ioPs4RIrk+++9K69ee8173ff6vXD+P1h+bBKbD20EoFFCIy5vejlXNL2Cy5pcRq049UiTsu1093EoOET8sHkzPP00jB3r3YFu5qiXlEPVOjs5mrCOHdGfcrTaGqj+He3OrUqvNhdwedPLuaThJcRGx4a6fBG/KDgUHBJAO3bAvHlea/f8j127CqwYuxeqf0dEzQwaND5Gapvq9Oz8K3p3bk5yUiQROlgsYUzBoeCQUpCZCZs2nQyS9f/OZuXa/Xy/ydi/KwFyo06sGxF9jDrJB2ndsjJtW8XRtCn86lfeo3FjqFw5dN9DBE4fHFGFrSwixRMf712Z1b593pJoIBHwuvMuX7ub6Z9/y5JVm/l2/WG276zD9pW/YuHCZrhjVU7sxwwaNDgZJAUfCQml/tVETtCIQyREnHNs2LuB+ZvmM++7+Sz8dg0Hd9SGn5pRL7sL1bM6435qyp4t1di9207ZtkaNkyHSsiW0a+fNP9K0qdcdWCQQdKhKwSFhLic3h2VblzFv0zzmb5rP51s+Jyc3h5ioGC6s1ZN2la4h+fil5O5tzKZNEScOh2VknJwuNzYWzjvPC5G2bU8GiibBlOJQcCg4pIw5eOwgSzKWeCOSTfNYs3sNADVja9KjaQ+uaHoFlze9nFrRjVm7Flavhm++Ofl3T76+jHXqnAyRvEBp3fqXbeVF8lNwKDikjNueuZ0F3y84MSLZlrkNgFpVatEgoQENqjWgYUJDGlRrQHK1BsQdbUrmlsZs+64ma1ZHsno1rF0LWVne/iIioHnzk4GS97dxY3S1lwAKDgWHlCvOOdbtWcf8TfP5dte3bD6wmc37N7P5wGYOHD1wyroRFkG9qvVokNCApLhGxB/sgNt5Hoe2NmXXprr88O94fvj+5HUyVasWfrirRo3S/pYSagoOBYdUEAeOHjgRIif+5nv+4/4fycrJOmWbqOzq1DrUjao/X4jtbM+Rrc34KaM+hw6cPJZVv76jXTs7JVBatdJlw+WZgkPBIQJ4o5W9R/aeMVy2HthK9vFsyKwPO9vCrrZE7u5I1O4Uju38FS6nEgCRkbkkNTlMm/OO06RBDAlVKxMbC1WqeCfqi/o8NhaidHNA2NF9HCICgJmRWCWRxCqJdKjXodB1cl0uOw/uPDVY9i9n84H3yNi7lYxNldi9qS7Hd53Hjzvb8ePi8+BIDciOgNzoYtUVHV30oDlTAJnB8eOQm/vLx+mWn+m94iyPivIO93XqBB06ePf3lCcacYhIsWQfz2Zb5rYT4bI1cytbD2xly/4dbN6zh+379rN9389kH42E7CqQHQs5sZBdhWqRdUmIrEs1q0vViESqkEiMq04ll0DU8WpYThwuuzKHDxtHjsCRI3D4ML94npV19jqLy8y7SCAy0vtb8HG65RERXl07d57cT8uW3kyTnTp5f1NSvHNJ4U4jDhEJqOjIaBqd04hG5zQ67Tp5h8W2ZW5j64GtbM3ceuL5toMr2XpgK99nbmXXoYJNvrzpe+vH16d+fH0axidRP74+SfFJJFU7+bxuXH0ic+NOBMqRI962xfkHP/9yM+9REjt3wooVsHy593fhQnjzTe89Mzj33JNB0qmTFyZxcSX7zNKiEYeIhNyx48fYcXCHFyiZ204GjG8Uk/f84LGDv9g2oXLCiTCpH1+fcyqfQ3zleKpWqkp8Jd/fM7yuFFkJK2lKFNH27V6I5AXK8uVew0zwAquwMKlS5cz7DCadHFdwiJR5B44eODliyR8sB08uO3D0AJnHMsl1uUXaZ1RE1GkDpqjhk/911UpVibCi3wizbdupI5Ply08e5oqI8G7UzH+Yq31771xOaVBwKDhEKgznHFk5WWQey+TgsYMcPHaQzKPe87xlv3h9uuW+10dyjhT586tEVyEuOo64SnGnPI+L9r32PS/4ukp0FapEx5H1Uw02r6/DprXV+ffqaqz7JpY9u70mZJGRjtat7ZQwadcuOGGi4FBwiEgJHM89fjKEzhIymccyOZx9mEPZhzh07BCHsg95r33P8y8reE9NoRxwIBm2dYLtqUTuSCN3ayfcoZre+xE5VE3KoPqvNlG72Wbqt9hOUvO9JFStzB+7/ZHKUcW72UbBoeAQkTB0PPf4iZApGC5nCp+DRw+xZ0cs2zbUY9fGhuzb1JTM71tw/FB1b8cR2VB7DTu+aUOdWsW7RFpXVYmIhKHIiEjiK8cTX7nkN3s4Bz/+mHe+JJp161KonRiAIgtQcIiIlBNm0KiR97juuuB9jnpgioiIXxQcIiLiFwWHiIj4RcEhIiJ+UXCIiIhfFBwiIuIXBYeIiPhFwSEiIn6pEC1HzGw38EOo6yihRGBPqIsIE/otTqXf41T6PU4q6W/RyDlXq+DCChEc5YGZLS+sZ0xFpN/iVPo9TqXf46Rg/RY6VCUiIn5RcIiIiF8UHGXHq6EuIIzotziVfo9T6fc4KSi/hc5xiIiIXzTiEBERvyg4RETELwqOMGZmDcxskZmtNbM1ZvbbUNcUDsws0sy+MrMPQ11LqJnZOWY2zczWm9k6M7sw1DWFipnd5/vv5Fsze9vMYkJdU2kys3+a2S4z+zbfshpmNs/M/uP7Wz0Qn6XgCG85wO+cc62BC4BRZtY6xDWFg98C60JdRJj4P2C2c64V0J4K+ruYWRJwD5DqnDsPiARuDG1VpW4C0LvAsgeBBc655sAC3+sSU3CEMefcdufcSt/zTLx/FJJCW1VomVky0AcYF+paQs3MEoBLgfEAzrljzrmfQ1tVSEUBsWYWBVQBtoW4nlLlnFsK/FRgcX9gou/5RGBAID5LwVFGmFljoAPwRWgrCbnngAeA3FAXEgaaALuB13yH7saZWVyoiwoF59xW4BngR2A7sN85Nze0VYWFOs657b7nO4A6gdipgqMMMLOqwDvAvc65A6GuJ1TMrC+wyzm3ItS1hIkooCPwknOuA3CIAB2KKGt8x+7744VpfSDOzG4ObVXhxXn3XgTk/gsFR5gzs2i80JjknHs31PWEWBegn5llAJOBy8zszdCWFFJbgC3OubxR6DS8IKmILge+d87tds5lA+8CF4W4pnCw08zqAfj+7grEThUcYczMDO/49Trn3N9CXU+oOececs4lO+ca4534XOicq7D/V+mc2wFsNrOWvkU9gLUhLCmUfgQuMLMqvv9uelBBLxQoYAYwzPd8GDA9EDtVcIS3LsBQvP+zXuV7XBXqoiSs3A1MMrNvgBTgryGuJyR8o65pwEpgNd6/bRWq9YiZvQ18DrQ0sy1mdjvwJHCFmf0Hb1T2ZEA+Sy1HRETEHxpxiIiIXxQcIiLiFwWHiIj4RcEhIiJ+UXCIiIhfFBwixWRmx/NdJr3KzAJ217aZNc7f5VQknESFugCRMuyIcy4l1EWIlDaNOEQCzMwyzOxpM1ttZl+aWTPf8sZmttDMvjGzBWbW0Le8jpm9Z2Zf+x55rTIizWysb46JuWYW61v/Ht8cLd+Y2eQQfU2pwBQcIsUXW+BQ1aB87+13zrUFXsDr6AvwD2Cic64dMAl43rf8eWCJc649Xq+pNb7lzYEXnXNtgJ+B63zLHwQ6+PZzZ7C+nMjp6M5xkWIys4POuaqFLM8ALnPObfI1qdzhnKtpZnuAes65bN/y7c65RDPbDSQ7547m20djYJ5vAh7M7A9AtHPuz2Y2GzgIvA+875w7GOSvKnIKjThEgsOd5rk/juZ7fpyT5yT7AC/ijU6W+SYuEik1Cg6R4BiU7+/nvuefcXI60yHAx77nC4CRcGI+9YTT7dTMIoAGzrlFwB+ABOAXox6RYNL/qYgUX6yZrcr3erZzLu+S3Oq+jrVHgcG+ZXfjzdb3X3gz993qW/5b4FVfN9PjeCGyncJFAm/6wsWA5yv4dLESAjrHIRJgvnMcqc65PaGuRSQYdKhKRET8ohGHiIj4RSMOERHxi4JDRET8ouAQERG/KDhERMQvCg4REfHL/wdGRYxOwwH6pwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Final validation loss : 0.0011842433596029878\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "LwkVNGQy5DVC",
        "outputId": "e893a382-9ac5-4244-8778-31147167c97b"
      },
      "source": [
        "acc_train = hist[6].history['accuracy']\n",
        "acc_validation = hist[6].history['val_accuracy']\n",
        "epochs = range(1, len(hist[6].history['accuracy']) + 1)\n",
        "plt.plot(epochs, acc_train, 'g', label='Training')\n",
        "plt.plot(epochs, acc_validation, 'b', label='Validation')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "print(f\"Final validation accuracy : {hist[6].history['val_accuracy'][9]:.3f}\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Final validation accuracy : 0.538\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}